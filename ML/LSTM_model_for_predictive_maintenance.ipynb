{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM model for predictive maintenance.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z_1vMUkrrdw"
      },
      "source": [
        "import tensorflow.keras\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os \n",
        "from tqdm.keras import TqdmCallback\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "np.random.seed(1234)  \n",
        "PYTHONHASHSEED = 0\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
        "from tensorflow.keras.models import Sequential,load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_7XA6NaryDC"
      },
      "source": [
        "# define path to save model\n",
        "model_path = 'predictive_maintenance_model.h5'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyvA_Lgar1I6"
      },
      "source": [
        "\n",
        "# read training data - It is the aircraft engine run-to-failure data.\n",
        "train_df = pd.read_csv('YOUR LOCATION----PM_train.txt', sep=\" \", header=None)\n",
        "train_df.drop(train_df.columns[[26, 27]], axis=1, inplace=True)\n",
        "train_df.columns = ['id', 'cycle', 'setting1', 'setting2', 'setting3', 's1', 's2', 's3',\n",
        "                     's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14',\n",
        "                     's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
        "\n",
        "train_df = train_df.sort_values(['id','cycle'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCmglxpLr-aN"
      },
      "source": [
        "# read ground truth data - It contains the information of true remaining cycles for each engine in the testing data.\n",
        "truth_df = pd.read_csv('YOUR LOCATION-----PM_truth.txt', sep=\" \", header=None)\n",
        "truth_df.drop(truth_df.columns[[1]], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvKnPM29sDwz"
      },
      "source": [
        "\n",
        "# TRAIN\n",
        "# Data Labeling - generate column RUL(Remaining Usefull Life or Time to Failure)\n",
        "rul = pd.DataFrame(train_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "train_df = train_df.merge(rul, on=['id'], how='left')\n",
        "train_df['RUL'] = train_df['max'] - train_df['cycle']\n",
        "train_df.drop('max', axis=1, inplace=True)\n",
        "# generate label columns for training data\n",
        "# we will only make use of \"label1\" for binary classification, \n",
        "# while trying to answer the question: is a specific engine going to fail within w1 cycles?\n",
        "w1 = 30\n",
        "w0 = 15\n",
        "train_df['label1'] = np.where(train_df['RUL'] <= w1, 1, 0 )\n",
        "train_df['label2'] = train_df['label1']\n",
        "train_df.loc[train_df['RUL'] <= w0, 'label2'] = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYJ9CfqRsGOg"
      },
      "source": [
        "# MinMax normalization (from 0 to 1)\n",
        "train_df['cycle_norm'] = train_df['cycle']\n",
        "cols_normalize = train_df.columns.difference(['id','cycle','RUL','label1','label2'])\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_df[cols_normalize]), \n",
        "                             columns=cols_normalize, \n",
        "                             index=train_df.index)\n",
        "join_df = train_df[train_df.columns.difference(cols_normalize)].join(norm_train_df)\n",
        "train_df = join_df.reindex(columns = train_df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxFxNHtJsIHs"
      },
      "source": [
        "\n",
        "# TEST\n",
        "# MinMax normalization (from 0 to 1)\n",
        "test_df['cycle_norm'] = test_df['cycle']\n",
        "norm_test_df = pd.DataFrame(min_max_scaler.transform(test_df[cols_normalize]), \n",
        "                            columns=cols_normalize, \n",
        "                            index=test_df.index)\n",
        "test_join_df = test_df[test_df.columns.difference(cols_normalize)].join(norm_test_df)\n",
        "test_df = test_join_df.reindex(columns = test_df.columns)\n",
        "test_df = test_df.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDxUxMoAsNiv"
      },
      "source": [
        "# We use the ground truth dataset to generate labels for the test data.\n",
        "# generate column max for test data\n",
        "rul = pd.DataFrame(test_df.groupby('id')['cycle'].max()).reset_index()\n",
        "rul.columns = ['id', 'max']\n",
        "truth_df.columns = ['more']\n",
        "truth_df['id'] = truth_df.index + 1\n",
        "truth_df['max'] = rul['max'] + truth_df['more']\n",
        "truth_df.drop('more', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K42JD4msPO5"
      },
      "source": [
        "# generate RUL for test data\n",
        "test_df = test_df.merge(truth_df, on=['id'], how='left')\n",
        "test_df['RUL'] = test_df['max'] - test_df['cycle']\n",
        "test_df.drop('max', axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO3PuDlzsSu5"
      },
      "source": [
        "# generate label columns w0 and w1 for test data\n",
        "test_df['label1'] = np.where(test_df['RUL'] <= w1, 1, 0 )\n",
        "test_df['label2'] = test_df['label1']\n",
        "test_df.loc[test_df['RUL'] <= w0, 'label2'] = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anw4sgJBsT6G"
      },
      "source": [
        "# LSTM\n",
        "# pick a large window size of 50 cycles\n",
        "sequence_length = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeQuMScdsWMU"
      },
      "source": [
        "# function to reshape features into (samples, time steps, features) \n",
        "def gen_sequence(id_df, seq_length, seq_cols):\n",
        "    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n",
        "    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n",
        "    we can use shorter ones \"\"\"\n",
        "    # for one id I put all the rows in a single matrix\n",
        "    data_matrix = id_df[seq_cols].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    # Iterate over two lists in parallel.\n",
        "    # For example id1 have 192 rows and sequence_length is equal to 50\n",
        "    # so zip iterate over two following list of numbers (0,112),(50,192)\n",
        "    # 0 50 -> from row 0 to row 50\n",
        "    # 1 51 -> from row 1 to row 51\n",
        "    # 2 52 -> from row 2 to row 52\n",
        "    # ...\n",
        "    # 111 191 -> from row 111 to 191\n",
        "    for start, stop in zip(range(0, num_elements-seq_length), range(seq_length, num_elements)):\n",
        "        yield data_matrix[start:stop, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9tMwu1esYHi"
      },
      "source": [
        "# pick the feature columns \n",
        "sensor_cols = ['s' + str(i) for i in range(1,22)]\n",
        "sequence_cols = ['setting1', 'setting2', 'setting3', 'cycle_norm']\n",
        "sequence_cols.extend(sensor_cols)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfaCNZ3WsZk-"
      },
      "source": [
        "# generator for the sequences\n",
        "seq_gen = (list(gen_sequence(train_df[train_df['id']==id], sequence_length, sequence_cols)) \n",
        "           for id in train_df['id'].unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvml_L4fsbR6"
      },
      "source": [
        "# generate sequences and convert to numpy array\n",
        "seq_array = np.concatenate(list(seq_gen)).astype(np.float32)\n",
        "seq_array.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_GLuGZoscvQ"
      },
      "source": [
        "# function to generate labels\n",
        "def gen_labels(id_df, seq_length, label):\n",
        "    data_matrix = id_df[label].values\n",
        "    num_elements = data_matrix.shape[0]\n",
        "    return data_matrix[seq_length:num_elements, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOqo72WCsib6"
      },
      "source": [
        "# generate labels\n",
        "label_gen = [gen_labels(train_df[train_df['id']==id], sequence_length, ['label1']) \n",
        "             for id in train_df['id'].unique()]\n",
        "label_array = np.concatenate(label_gen).astype(np.float32)\n",
        "label_array.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvhgTUsFsj4_"
      },
      "source": [
        "\n",
        "# Next, we build a deep network. \n",
        "# The first layer is an LSTM layer with 100 units followed by another LSTM layer with 50 units. \n",
        "# Dropout is also applied after each LSTM layer to control overfitting. \n",
        "# Final layer is a Dense output layer with single unit and sigmoid activation since this is a binary classification problem.\n",
        "# build the network\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(\n",
        "         input_shape=(sequence_length, nb_features),\n",
        "         units=100,\n",
        "         return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(\n",
        "          units=50,\n",
        "          return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(units=nb_out, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6hL5W1lsnGT"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVLO7-xosoio"
      },
      "source": [
        "\n",
        "# fit the network\n",
        "history = model.fit(seq_array, label_array, epochs=100, batch_size=200, validation_split=0.05, verbose=2,\n",
        "          callbacks = [tensorflow.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='min'),\n",
        "                       tensorflow.keras.callbacks.ModelCheckpoint(model_path,monitor='val_loss', save_best_only=True, mode='min', verbose=0),\n",
        "                       TqdmCallback(verbose=2)]\n",
        "          )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSKvkATksqp_"
      },
      "source": [
        "print(history.history.keys())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn3b2hiFstDZ"
      },
      "source": [
        "\n",
        "# summarize history for Accuracy\n",
        "fig_acc = plt.figure(figsize=(10, 10))\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "fig_acc.savefig(\"model_accuracy.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4SPeTh0svy7"
      },
      "source": [
        "\n",
        "# summarize history for Loss\n",
        "fig_acc = plt.figure(figsize=(10, 10))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "fig_acc.savefig(\"model_loss.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_0cXbwiswd-"
      },
      "source": [
        "# training metrics\n",
        "scores = model.evaluate(seq_array, label_array, verbose=1, batch_size=200)\n",
        "print('Accurracy: {}'.format(scores[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgPsJAybsyZV"
      },
      "source": [
        "# make predictions and compute confusion matrix\n",
        "y_pred = model.predict_classes(seq_array,verbose=1, batch_size=200)\n",
        "y_true = label_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn_B9UVKsz9l"
      },
      "source": [
        "test_set = pd.DataFrame(y_pred)\n",
        "test_set.to_csv('binary_submit_train.csv', index = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_ecxTaGs1j4"
      },
      "source": [
        "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWzoKU2js254"
      },
      "source": [
        "# compute precision and recall\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "print( 'precision = ', precision, '\\n', 'recall = ', recall)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7n68bkzHs4Us"
      },
      "source": [
        "# EVALUATE ON TEST DATA\n",
        "\n",
        "# We pick the last sequence for each id in the test data\n",
        "\n",
        "seq_array_test_last = [test_df[test_df['id']==id][sequence_cols].values[-sequence_length:] \n",
        "                       for id in test_df['id'].unique() if len(test_df[test_df['id']==id]) >= sequence_length]\n",
        "\n",
        "seq_array_test_last = np.asarray(seq_array_test_last).astype(np.float32)\n",
        "print(\"seq_array_test_last\")\n",
        "print(seq_array_test_last)\n",
        "print(seq_array_test_last.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEtXxjE8s9ri"
      },
      "source": [
        "# Similarly, we pick the labels\n",
        "\n",
        "# serve per prendere solo le label delle sequenze che sono almeno lunghe 50\n",
        "y_mask = [len(test_df[test_df['id']==id]) >= sequence_length for id in test_df['id'].unique()]\n",
        "print(\"y_mask\")\n",
        "print(y_mask)\n",
        "label_array_test_last = test_df.groupby('id')['label1'].nth(-1)[y_mask].values\n",
        "label_array_test_last = label_array_test_last.reshape(label_array_test_last.shape[0],1).astype(np.float32)\n",
        "print(label_array_test_last.shape)\n",
        "print(\"label_array_test_last\")\n",
        "print(label_array_test_last)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NsdpzRBs-lH"
      },
      "source": [
        "# if best iteration's model was saved then load and use it\n",
        "if os.path.isfile(model_path):\n",
        "    estimator = load_model(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAZGvCy2tAwk"
      },
      "source": [
        "# test metrics\n",
        "scores_test = estimator.evaluate(seq_array_test_last, label_array_test_last, verbose=2)\n",
        "print('Accuracy: {}'.format(scores_test[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC3zqyYWtCXS"
      },
      "source": [
        "# make predictions and compute confusion matrix\n",
        "y_pred_test = estimator.predict_classes(seq_array_test_last)\n",
        "y_true_test = label_array_test_last"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkXd13L5tQAV"
      },
      "source": [
        "test_set = pd.DataFrame(y_pred_test)\n",
        "test_set.to_csv('binary_submit_test.csv', index = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBsckB9_tRzI"
      },
      "source": [
        "print('Confusion matrix\\n- x-axis is true labels.\\n- y-axis is predicted labels')\n",
        "cm = confusion_matrix(y_true_test, y_pred_test)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvYc_l-3tT9x"
      },
      "source": [
        "# compute precision and recall\n",
        "precision_test = precision_score(y_true_test, y_pred_test)\n",
        "recall_test = recall_score(y_true_test, y_pred_test)\n",
        "f1_test = 2 * (precision_test * recall_test) / (precision_test + recall_test)\n",
        "print( 'Precision: ', precision_test, '\\n', 'Recall: ', recall_test,'\\n', 'F1-score:', f1_test )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tAGDjlFtVyc"
      },
      "source": [
        "\n",
        "# Plot in blue color the predicted data and in green color the\n",
        "# actual data to verify visually the accuracy of the model.\n",
        "fig_verify = plt.figure(figsize=(100, 50))\n",
        "plt.plot(y_pred_test, color=\"blue\")\n",
        "plt.plot(y_true_test, color=\"green\")\n",
        "plt.title('prediction')\n",
        "plt.ylabel('value')\n",
        "plt.xlabel('row')\n",
        "plt.legend(['predicted', 'actual data'], loc='upper left')\n",
        "plt.show()\n",
        "fig_verify.savefig(\"model_verify.png\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSQKsWIntg91"
      },
      "source": [
        "#FOR TF LITE\n",
        "# tflite_model_file = \"predictive_maintenance_converted_model.tflite\"\n",
        "# model = load_model(model_path)\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "# tflite_model = converter.convert()\n",
        "# open(tflite_model_file, \"wb\").write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}